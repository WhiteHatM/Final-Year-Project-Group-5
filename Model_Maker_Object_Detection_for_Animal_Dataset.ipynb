{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2iEwAriW51W"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanhlvg/tflite_raspberry_pi/blob/main/object_detection/Train_custom_model_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJEzDG6DK2Q"
      },
      "source": [
        "# Train a custom animal detection model with TensorFlow Lite Model Maker\n",
        "\n",
        "In this colab notebook, we'll use the [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/guide/model_maker) to train a custom object detection model to detect Animals.\n",
        "\n",
        "The Model Maker library uses *transfer learning* to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with our own custom dataset reduces the amount of training data required and will shorten the training time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRYjtwRZGBOI"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "### Install the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35BJmtVpAP_n",
        "outputId": "59a07366-a593-4c50-8a35-02172f55a445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 64.6 kB of archives.\n",
            "After this operation, 215 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Fetched 64.6 kB in 1s (72.1 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 155629 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sounddevice in /usr/local/lib/python3.7/dist-packages (0.4.4)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.7/dist-packages (from sounddevice) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from CFFI>=1.0->sounddevice) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q tflite-support\n",
        "!sudo apt-get install libportaudio2\n",
        "!pip install sounddevice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prQ86DdtD317"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4QQTXHHATDS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "from tflite_support import metadata\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g6aQvXsD78P"
      },
      "source": [
        "### Prepare the dataset\n",
        "\n",
        "This dataset contains about 70 images of 2 type of model animals: a Cow and a Hen. This is an example image from the dataset.\n",
        "\n",
        "![animals_sample.jpg](https://drive.google.com/file/d/1HoWICSs1GIuZKdzo0HqXxEZkG3Likc4U/view?usp=sharing)\n",
        "\n",
        "We start with downloading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AGg7D4JAV62",
        "outputId": "f4889722-34f0-4070-b90b-14dc0ee8902d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace animals/train/1653474187132.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ],
      "source": [
        "!unzip -q animals.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "del1cg2RYLpA",
        "outputId": "ad35fdcd-5022-433f-cb9c-7118ff0172c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 124\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxh3KInCFeB-"
      },
      "source": [
        "## Train the object detection model\n",
        "\n",
        "### Step 1: Load the dataset\n",
        "\n",
        "* Images in `train_data` is used to train the custom object detection model.\n",
        "* Images in `val_data` is used to check if the model can generalize well to new images that it hasn't seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiAahdsQAdT7"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'animals/train',\n",
        "    'animals/train',\n",
        "    ['cow', 'hen']\n",
        ")\n",
        "\n",
        "val_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'animals/validate',\n",
        "    'animals/validate',\n",
        "    ['cow', 'hen']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNRhB8N7GHXj"
      },
      "source": [
        "### Step 2: Select a model architecture\n",
        "\n",
        "EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n",
        "\n",
        "In this notebook, we use EfficientDet-Lite0 to train our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZOojrDHAY1J"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aeDU4mIM4ft"
      },
      "source": [
        "### Step 3: Train the TensorFlow model with the training data.\n",
        "\n",
        "* Set `epochs = 20`, which means it will go through the training dataset 20 times.\n",
        "* Set `batch_size = 4` here so you will see that it takes 15 steps to go through the 62 images in the training dataset.\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MClfpsJAfda",
        "outputId": "05f35b8c-6273-4e01-8649-f999809edaf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "16/16 [==============================] - 67s 2s/step - det_loss: 1.7137 - cls_loss: 1.1063 - box_loss: 0.0121 - reg_l2_loss: 0.0630 - loss: 1.7767 - learning_rate: 0.0065 - gradient_norm: 1.6643 - val_det_loss: 1.5727 - val_cls_loss: 1.0486 - val_box_loss: 0.0105 - val_reg_l2_loss: 0.0630 - val_loss: 1.6356\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 24s 2s/step - det_loss: 1.5555 - cls_loss: 1.0076 - box_loss: 0.0110 - reg_l2_loss: 0.0630 - loss: 1.6185 - learning_rate: 0.0049 - gradient_norm: 2.2923 - val_det_loss: 1.2414 - val_cls_loss: 0.7979 - val_box_loss: 0.0089 - val_reg_l2_loss: 0.0630 - val_loss: 1.3044\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 27s 2s/step - det_loss: 1.2047 - cls_loss: 0.7602 - box_loss: 0.0089 - reg_l2_loss: 0.0630 - loss: 1.2677 - learning_rate: 0.0048 - gradient_norm: 2.6795 - val_det_loss: 1.1105 - val_cls_loss: 0.7254 - val_box_loss: 0.0077 - val_reg_l2_loss: 0.0630 - val_loss: 1.1735\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 30s 2s/step - det_loss: 1.0670 - cls_loss: 0.6460 - box_loss: 0.0084 - reg_l2_loss: 0.0630 - loss: 1.1300 - learning_rate: 0.0046 - gradient_norm: 3.5220 - val_det_loss: 0.8203 - val_cls_loss: 0.4904 - val_box_loss: 0.0066 - val_reg_l2_loss: 0.0630 - val_loss: 0.8833\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 36s 2s/step - det_loss: 0.8742 - cls_loss: 0.5167 - box_loss: 0.0071 - reg_l2_loss: 0.0630 - loss: 0.9372 - learning_rate: 0.0043 - gradient_norm: 2.8873 - val_det_loss: 0.7461 - val_cls_loss: 0.4446 - val_box_loss: 0.0060 - val_reg_l2_loss: 0.0630 - val_loss: 0.8091\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 24s 2s/step - det_loss: 0.7402 - cls_loss: 0.4335 - box_loss: 0.0061 - reg_l2_loss: 0.0630 - loss: 0.8032 - learning_rate: 0.0040 - gradient_norm: 2.9003 - val_det_loss: 0.7614 - val_cls_loss: 0.4925 - val_box_loss: 0.0054 - val_reg_l2_loss: 0.0630 - val_loss: 0.8244\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 27s 2s/step - det_loss: 0.6806 - cls_loss: 0.3974 - box_loss: 0.0057 - reg_l2_loss: 0.0630 - loss: 0.7436 - learning_rate: 0.0037 - gradient_norm: 3.1944 - val_det_loss: 0.5483 - val_cls_loss: 0.3050 - val_box_loss: 0.0049 - val_reg_l2_loss: 0.0630 - val_loss: 0.6113\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 28s 2s/step - det_loss: 0.6059 - cls_loss: 0.3590 - box_loss: 0.0049 - reg_l2_loss: 0.0630 - loss: 0.6689 - learning_rate: 0.0033 - gradient_norm: 2.9365 - val_det_loss: 0.5149 - val_cls_loss: 0.2969 - val_box_loss: 0.0044 - val_reg_l2_loss: 0.0630 - val_loss: 0.5780\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 27s 2s/step - det_loss: 0.5991 - cls_loss: 0.3449 - box_loss: 0.0051 - reg_l2_loss: 0.0630 - loss: 0.6621 - learning_rate: 0.0029 - gradient_norm: 3.9756 - val_det_loss: 0.4662 - val_cls_loss: 0.2606 - val_box_loss: 0.0041 - val_reg_l2_loss: 0.0630 - val_loss: 0.5293\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 36s 2s/step - det_loss: 0.5408 - cls_loss: 0.3290 - box_loss: 0.0042 - reg_l2_loss: 0.0630 - loss: 0.6039 - learning_rate: 0.0025 - gradient_norm: 2.5844 - val_det_loss: 0.4678 - val_cls_loss: 0.2713 - val_box_loss: 0.0039 - val_reg_l2_loss: 0.0630 - val_loss: 0.5309\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 25s 2s/step - det_loss: 0.5387 - cls_loss: 0.3288 - box_loss: 0.0042 - reg_l2_loss: 0.0630 - loss: 0.6018 - learning_rate: 0.0021 - gradient_norm: 3.1673 - val_det_loss: 0.4286 - val_cls_loss: 0.2587 - val_box_loss: 0.0034 - val_reg_l2_loss: 0.0631 - val_loss: 0.4917\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 27s 2s/step - det_loss: 0.5354 - cls_loss: 0.3406 - box_loss: 0.0039 - reg_l2_loss: 0.0631 - loss: 0.5985 - learning_rate: 0.0017 - gradient_norm: 2.8872 - val_det_loss: 0.3857 - val_cls_loss: 0.2244 - val_box_loss: 0.0032 - val_reg_l2_loss: 0.0631 - val_loss: 0.4487\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 28s 2s/step - det_loss: 0.4879 - cls_loss: 0.3021 - box_loss: 0.0037 - reg_l2_loss: 0.0631 - loss: 0.5510 - learning_rate: 0.0013 - gradient_norm: 2.7541 - val_det_loss: 0.3536 - val_cls_loss: 0.2092 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.0631 - val_loss: 0.4167\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 28s 2s/step - det_loss: 0.4551 - cls_loss: 0.2786 - box_loss: 0.0035 - reg_l2_loss: 0.0631 - loss: 0.5182 - learning_rate: 9.6756e-04 - gradient_norm: 2.8400 - val_det_loss: 0.3530 - val_cls_loss: 0.2104 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.0631 - val_loss: 0.4161\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 36s 2s/step - det_loss: 0.4483 - cls_loss: 0.2798 - box_loss: 0.0034 - reg_l2_loss: 0.0631 - loss: 0.5113 - learning_rate: 6.6399e-04 - gradient_norm: 2.5864 - val_det_loss: 0.3509 - val_cls_loss: 0.2157 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0631 - val_loss: 0.4140\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 24s 2s/step - det_loss: 0.4865 - cls_loss: 0.2958 - box_loss: 0.0038 - reg_l2_loss: 0.0631 - loss: 0.5496 - learning_rate: 4.1050e-04 - gradient_norm: 3.0367 - val_det_loss: 0.3475 - val_cls_loss: 0.2122 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0631 - val_loss: 0.4106\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 32s 2s/step - det_loss: 0.4744 - cls_loss: 0.3098 - box_loss: 0.0033 - reg_l2_loss: 0.0631 - loss: 0.5375 - learning_rate: 2.1400e-04 - gradient_norm: 2.5192 - val_det_loss: 0.3478 - val_cls_loss: 0.2110 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0631 - val_loss: 0.4108\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 25s 2s/step - det_loss: 0.4564 - cls_loss: 0.2847 - box_loss: 0.0034 - reg_l2_loss: 0.0631 - loss: 0.5194 - learning_rate: 7.9862e-05 - gradient_norm: 2.6617 - val_det_loss: 0.3411 - val_cls_loss: 0.2068 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0631 - val_loss: 0.4042\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 27s 2s/step - det_loss: 0.4327 - cls_loss: 0.2759 - box_loss: 0.0031 - reg_l2_loss: 0.0631 - loss: 0.4957 - learning_rate: 1.1738e-05 - gradient_norm: 2.5871 - val_det_loss: 0.3423 - val_cls_loss: 0.2089 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0631 - val_loss: 0.4054\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 36s 2s/step - det_loss: 0.4345 - cls_loss: 0.2703 - box_loss: 0.0033 - reg_l2_loss: 0.0631 - loss: 0.4976 - learning_rate: 1.1488e-05 - gradient_norm: 2.4332 - val_det_loss: 0.3397 - val_cls_loss: 0.2074 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.0631 - val_loss: 0.4028\n"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=4, train_whole_model=True, epochs=20, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB4hKeerMmh4"
      },
      "source": [
        "### Step 4. Evaluate the model with the validation data.\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the 10 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 1 step to go through the 10 images in the validation dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUqEpcYwAg8L",
        "outputId": "47b324ee-a8d2-403e-aa96-06c725191bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r1/1 [==============================] - 9s 9s/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.7393187,\n",
              " 'AP50': 1.0,\n",
              " 'AP75': 1.0,\n",
              " 'AP_/cow': 0.6981094,\n",
              " 'AP_/hen': 0.78052807,\n",
              " 'APl': 0.7393187,\n",
              " 'APm': -1.0,\n",
              " 'APs': -1.0,\n",
              " 'ARl': 0.785,\n",
              " 'ARm': -1.0,\n",
              " 'ARmax1': 0.77,\n",
              " 'ARmax10': 0.785,\n",
              " 'ARmax100': 0.785,\n",
              " 'ARs': -1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model.evaluate(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NARVYk9rGLIl"
      },
      "source": [
        "### Step 5: Export as a TensorFlow Lite model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3eFxoBAiqE"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', tflite_filename='animals.tflite')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cC7VT5aO5mFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcBmEigOCO3"
      },
      "source": [
        "### Step 6:  Evaluate the TensorFlow Lite model.\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore we'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbl8z9_wBPlr",
        "outputId": "fe1c6fc0-bf9c-455c-a3c4-b5e078b90e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 31s 2s/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.75148517,\n",
              " 'AP50': 1.0,\n",
              " 'AP75': 1.0,\n",
              " 'AP_/cow': 0.7029703,\n",
              " 'AP_/hen': 0.8,\n",
              " 'APl': 0.75148517,\n",
              " 'APm': -1.0,\n",
              " 'APs': -1.0,\n",
              " 'ARl': 0.775,\n",
              " 'ARm': -1.0,\n",
              " 'ARmax1': 0.775,\n",
              " 'ARmax10': 0.775,\n",
              " 'ARmax100': 0.775,\n",
              " 'ARs': -1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model.evaluate_tflite('animals.tflite', val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7zgUkdOUUnD",
        "outputId": "7fdcd211-130a-4773-da98-6bb9ac0e3278"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-830a31557898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download the TFLite model to the local computer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'animals.tflite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: animals.tflite"
          ]
        }
      ],
      "source": [
        "# Download the TFLite model to the local computer.\n",
        "from google.colab import files\n",
        "files.download('animals.tflite')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Model Maker Object Detection for Animal Dataset",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}